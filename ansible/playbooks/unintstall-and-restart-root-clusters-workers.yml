# Kill everything
- hosts: nodes
  roles:
    - kill-all-workers

- hosts: one_doc
  tasks:
    - name: Kill One-DOC components
      command: "docker compose -f {{ oak_repo_path }}/run-a-cluster/1-DOC.yaml kill"
      become: True

- hosts: root
  tasks:
    - name: Kill Root components
      command: "docker compose -f {{ oak_repo_path }}/root_orchestrator/docker-compose.yml kill"
      become: True

- hosts: cluster
  tasks:
    - name: Kill Cluster components
      command: "docker compose -f {{ oak_repo_path }}/cluster_orchestrator/docker-compose.yml  kill"
      become: True

# restart machines
- hosts: root,cluster
  become: true
  roles:
    - cleanup-root-cluster

- hosts: nodes
  become: true
  roles:
    - cleanup-node-files

# re-install everything
- hosts: root
  vars:
    repo: "{{ oak_repo_link }}"
    path: "{{ oak_repo_path }}"
    branch: "{{ oak_repo_version }}"
  roles:
    - ensure-oakestra-repo-is-cloned 
    - run-root

- hosts: cluster
  roles:
    - role: ensure-oakestra-repo-is-cloned
      vars:
        repo: "{{ oak_repo_link }}"
        path: "{{ oak_repo_path }}"
        branch: "{{ oak_repo_version }}"
    - role: run-cluster

- hosts: nodes
  roles:
    - setup-net-manager
    - setup-node-engine

- hosts: nodes
  roles:
    - kill-all-workers

  tasks:
    - name: Restart Net Manager
      shell: 'nohup /bin/NetManager -p 6000 {{ additionalNetManagerParams }} </dev/null >/tmp/netmanager.log 2>&1 &'
      become: true

    - name: Restart Node Engine
      environment:
        CLUSTER_IP: '{{ clusterIP }}'
      shell: 'nohup /bin/NodeEngine -n 6000 -a $CLUSTER_IP -p 10100 {{ additionalNodeEngineParams }} </dev/null >/tmp/nodeengine.log 2>&1 &'
      become: true
      when:  not ( proxy | default(false) )